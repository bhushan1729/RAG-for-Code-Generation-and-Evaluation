# RAG for Code Generation

This project implements a Retrieval-Augmented Generation (RAG) system for automated code generation. It leverages test steps to generate a vector database, retrieves similar test cases, fetches the code of similar test cases, generates query test case code using a Large Language Model (LLM), and refines the generated code iteratively using the comments and issues generated by using the LLM as a judge.

---

## Directory Structure
```
RAG for Code Generation/
├── data/
│ ├── query_test_cases.txt # File containing test cases and steps for which code has to be generated
│ ├── test_steps.txt # Input for creating the vector database
│ └── test_cases_code/ # Folder containing .py files for test case codes
├── modelfiles/
│ ├── first_iteration_code_generation # Model file for the first iteration of code generation
│ ├── second_iteration_code_eneration # Model file for the second iteration of code generation
│ └── llm_as_a_judge # Model file for evaluating generated code
├── prompts/
│ ├── first_iteration_code_gen_prompt.txt # Prompt for first iteration of code generation
│ ├── second_iteration_code_gen_prompt.txt # Prompt for second iteration of code generation
│ └── llm_as_a_judge_prompt.txt # Prompt for evaluating generated code
├── src/
│ ├── create_vector_db.py # Script to create the vector database
│ ├── first_iteration_code_gen.py # Script for first iteration code generation
│ ├── second_iteration_code_gen.py # Script for second iteration code generation
│ ├── llm_as_a_judge.py # Script for evaluating code
│ └── utils.py # Utility functions for data processing
├── temp.txt # File to store entire prompt and context
├── README.md # Project documentation
└── requirements.txt # Python dependencies
```

# Features

1. **Retrieval-Augmented Generation (RAG)**:
    *   Builds a vector database from  `data/test_steps.txt`.
    *   Retrieves similar test cases to the query and corresponding code of test cases for generating code of query test cases.
2. **Code Generation**:
    *   First iteration: Generates initial test case code based on three retrieved examples.
    *   Second iteration: Refines the generated code using additional context.
3. **Code Evaluation**:
    *   The LLM evaluates the generated code to ensure correctness and provides feedback for improvement.

---

# How to Use

## 1. Prepare the Dataset

### Guidelines for creating dataset:

For creating `test_steps.txt` and `query_test_cases.txt` files write the test cases in the following format:

```
Name: name_of_test_case
Summary: summary # Summary of test case if available
Test Steps:
Step 1:
Step 2:
and so on
```

*   Separate the information of each test case by two empty lines (or whatever marker you want to use). Two empty lines will be used as a marker for chunking and creating the vector database of test cases. The same marker will be used in getting query test cases from the  `query_test_cases.txt`  file for which code will be generated.
*   In the  `test_case_code`  folder,  `.py`  files of test cases should be saved. The name of the files should be the same which is after the  `Name:`  field. Therefore, files in the  `test_case_code`  will be named as  `name_of_test_case.py`.
*   Place the input test cases and steps as in the above format in  `data/test_steps.txt`.
*   Place the query test cases and steps as mentioned in the above format in  `data/query_test_cases.txt`.
*   Store the test case code files in  `data/test_cases_code/`.

## 2. Prepare the Models

*   Download the model from  [Ollama](https://ollama.com/)  to your local machine. The command for downloading models is

    ```
    ollama pull <name_of_model>
    ```

    For specific size and quantization, go to  [ollama.com](https://ollama.com/). More information about using models and commands is given here: [Ollama GitHub](https://github.com/ollama/ollama).
    For customizing the models by changing hyperparameters and system prompt use the following command. Information of the hyperparameters and system prompt should be stored in  `Modelfile`.

    ```
    ollama create example -f Modelfile
    ```

    For running the model

    ```
    ollama run <name_of_model>
    ```

    To see the hyperparameters and other information of your model

    ```
    ollama show <name_of_model>
    ```

    *   **first_iteration_code_generation**: Modelfile used for the initial code generation step.
    *   **second_iteration_code_generation**: Modelfile used for refining code generation by taking LLM judge feedback.
    *   **llm_as_a_judge**: Modelfile used to evaluate and improve the generated code.

## 3. Run the Pipeline

*   **Create Vector Database**:
    Run the  `create_vector_db.py`  script.
    *   Input:  `data/test_steps.txt`
    *   Output: Folder will be created in  `/src`  with user given name

*   **First Iteration Code Generation**:
    Run the  `src/first_iteration_code_gen.py`  script.
    *   Input:
        1. `data/query_test_cases.txt`
        2. Path to vector database
        3. `data/test_cases_code/`
        4. File path of temp.txt to store the entire prompt and context before sending to LLM
        5. Path of folder where user wants to save the output of generated code
    *   Output: Entire prompt, context example, and generated code will be saved at the user-given path of the folder in  `test_case_name_timestamp.txt` file
    *   Flow: First, three similar test cases will be fetched from the vector database and then corresponding Python code from  `data/test_cases_code/`. This with the initial prompt will be sent to the LLM for the first iteration of code generation.

*   **Second Iteration Code Generation**:
    Run the  `src/second_iteration_code_gen.py`  script.
    *   Input:
        1. Path of the folder where code generated in the first iteration is stored.
        2. Path of the folder where the review of the first iteration code is stored.
        3. Path to vector database
        4. Path of the folder where the user wants to save the output of generated code
        5. File path of temp.txt to store the entire prompt and context before sending to LLM
    *   Output: Entire prompt, context example, and generated code will be saved at the user-given path of the folder in  `test_case_name_timestamp.txt` file
    *   Flow: One most similar test case for a given query + code generated after the first iteration + comments and issues after the first iteration code will be sent to the LLM to refine the code

*   **Code Evaluation**:
    Run the  `src/llm_as_a_judge.py`  script.
    *   Input:
        1. `data/query_test_cases.txt`
        2. Path to vector database
        3. Path of the folder where generated code is stored
        4. Path of the folder where the user wants to save the output of reviews
    *   Output: Review of each generated code is stored at the user-given folder in  `name_of_test_case_timestamp.txt`  file.
    *   Flow: One most similar test case with code and code query steps with code generated by LLM is given to LLM judge to point out issues and give comments

---

## Prompts

The  `prompts/`  folder contains predefined prompt templates for various stages of the pipeline. These prompts are already used while generating code as well as reviewing. These files include the format in which input is going to LLM.

1. `first_iteration_code_gen_prompt.txt`: Template for initial code generation.
2. `second_iteration_code_gen_prompt.txt`: Template for refining code.
3. `llm_as_a_judge_prompt.txt`: Template for evaluating code.

---

## Dependencies

Include the following Python packages in your  `requirements.txt`  file:
